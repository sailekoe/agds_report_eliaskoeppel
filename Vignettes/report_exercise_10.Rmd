---
title: "report_exercise_10"
author: "Elias KÃ¶ppel"
date: "2023-05-20"
output: html_document
---




```{r}
library(dplyr)
library(tidyverse)
library(cowplot)
library(ggplot2)
library(recipes)
source("report_exercise_10_functions.R")




davos_flux <- read_csv("C:/Users/elias/OneDrive/Uni/4. Semester/Geo/AGDS I/agds_report_eliaskoeppel/data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv")

davos_flux <- get_selection(davos_flux)
davos_flux <- davos_flux[, -c(1,8)] #getting rid of P_F because I want to pool the davos data with the laegern data later and in the laegern data all the P_F values are NA. Also I get rid of the Timestamp column because in my laegern data set this coumn does not exist...

laegern_flux <- read_csv("C:/Users/elias/OneDrive/Uni/4. Semester/Geo/AGDS I/agds_report_eliaskoeppel/data/FLX_CH-Lae_FLUXNET2015_FULLSET_DD_2004-2014_1-4.csv")

laegern_flux <- get_selection_laegern(laegern_flux)
laegern_flux <- laegern_flux[,-7] #removing the seventh coulumn because it only consists of NA values

set.seed(1982) 


#getting splits for davos data
split1 <- rsample::initial_split(davos_flux, prop = 0.8, strata = "VPD_F")
  
davos_flux_train <- rsample::training(split1)
  
davos_flux_test <- rsample::testing(split1)


#getting splits for laegern data
split2 <- rsample::initial_split(laegern_flux, prop = 0.8, strata = "VPD_F")

laegern_flux_train <- rsample::training(split2)
  
laegern_flux_test <- rsample::testing(split2)


# pre-processing with the get_pp function I stored in the extra file
pp_davos_train <- get_pp(davos_flux_train)
pp_davos_test <- get_pp(davos_flux_test)

pp_laegern_train <- get_pp(laegern_flux_train)
pp_laegern_teset <- get_pp(laegern_flux_test)




```




k-fold cross validation for the davos_flux_train data:
```{r}
set.seed(1982)
mod_cv_davos_train <- caret::train(pp_davos_train, 
                       data = davos_flux_train |> drop_na(), 
                       method = "knn",
                       trControl = caret::trainControl(method = "cv", number = 10))
                       




```



k-fold cross validation for the laegern_flux_train data:
```{r warning = FALSE}
set.seed(1982)
mod_cv_laegern_train <- caret::train(pp_laegern_train, 
                       data = laegern_flux_train |> drop_na(), 
                       method = "knn",
                       trControl = caret::trainControl(method = "cv", number = 10))
                       




```









How the cv model of the davos train set performs on the davos test set (within-site)

```{r}
plot_1 <- eval_model(mod = mod_cv_davos_train, df_test = davos_flux_test)

```


How the cv model of the davos train set performs on the laegern test set (across-site)
```{r warning = FALSE}
plot_2 <- eval_model(mod = mod_cv_davos_train, df_test = laegern_flux_test)

```


In the following plot we see: 

On the left: how the model of the davos train set performs on the davos test set (within-site)
On the right: how the model of the davos train set performs on the laegern test set (across-site)
```{r}
cowplot::plot_grid(plot_1, plot_2) 

```
How the cv model of the laegern train set performs on the laegern test set (within-site)
```{r}
plot_3 <- eval_model(mod = mod_cv_laegern_train, df_test = laegern_flux_test)

```

How the cv model of the laegern train set performs on the davos test set (across-site)
```{r}
plot_4 <- eval_model(mod = mod_cv_laegern_train, df_test = davos_flux_test)

```







In the following plot we see: 

On the left: how the model of the laegern train set performs on the laegern test set (within-site)
On the right: how the model of the laegern train set performs on the davos test set (across-site)
```{r}
cowplot::plot_grid(plot_3, plot_4) 
```




pooling data and creating subsets, with which I create a single model
```{r warning = FALSE}

data_pooled <- rbind(davos_flux, laegern_flux)




set.seed(1982) 


#getting splits for davos data
split3 <- rsample::initial_split(data_pooled, prop = 0.8, strata = "VPD_F")
  
data_pooled_train <- rsample::training(split3)
  
data_pooled_test <- rsample::testing(split3)





# pre-processing with the get_pp function I stored in the extra file
pp_data_pooled_train <- get_pp(data_pooled_train)
pp_data_pooled_test <- get_pp(data_pooled_test)




#creating model
set.seed(1982)
mod_data_pooled_train <- caret::train(pp_data_pooled_train, 
                       data = data_pooled_train |> drop_na(), 
                       method = "knn",
                       trControl = caret::trainControl(method = "cv", number = 10))




```





applying the pooled model to the test data sets of both sites
```{r}

plot_5 <- eval_model(mod = mod_data_pooled_train, df_test = davos_flux_test)

```


```{r}
plot_6 <- eval_model(mod = mod_data_pooled_train, df_test = laegern_flux_test)
```


In the following plot we see: 

On the left: how the model of the pooled data set performs on the davos test set
On the right: how the model of the pooled data set performs on the leagern test set 
```{r}
cowplot::plot_grid(plot_5, plot_6) 
```


Interpretation of differences between the performance of the pooled data model and the true out-of-sample setup above. 



Concerning the r-squared value: I can  observe a small difference between the r-squared value of the within-site predictions (0.65 and 0.64) and the predictions of the pooled data model (0.66, 0.7). This means that in this case the model which was trained with pooled data performs better on the test data set of each site than the models which were trained on the site-specific training set. To me this is surprising because I would have assumed that the within-site prediction would get the best results. 


Concerning the RMSE value: Also for the RMSE value I can not observe a clear tendency: While for the davos test set I get a better result (lower RMSE) with the model that was trained with with the local data set than with the model based on the pooled data set (1.54 vs. 1.68), it is the other way round for the Laegern test set (2.46 vs. 2.33). Also here I am surprised because I would have expected the within-site prediction to achieve the best results. 


Concerning the question if it is valid approach to perform model training like this regarding the structure in the data: 
In the tutorial we learned that a fundamental assumption underlying many machine learning algorithms is that the data are independent and identically distributed (iid). This means that each observation is generated by the same process and follows the same distribution. Given the fact that we are looking at two different sites here which differ a lot concerning the vegetation and therefore also regarding the processes (and their variation in time), I would not expect the data to be identically distributed. As a consequence, I used folds for cross-validation that respect group delineations. So I think this approach is a valid one. 


Site characteristics: 

Davos: 
  Elevation: 1639
  Climate Koeppen: 	ET (Tundra)
  Mean Annual Temp (?C): 	2.8
  Mean Annual Precip. (mm): 	1062
  Vegetation IGBP: 	ENF (Evergreen Needleleaf Forests: Lands dominated by woody vegetation with a       percent cover >60% and height exceeding 2 meters. Almost all trees remain green all year. Canopy is   never without green foliage.)
  


  
Laegern: 
  Elevation: 689
  Climate Koeppen: 	--
  Mean Annual Temp (?C): 	8.3
  Mean Annual Precip. (mm): 	1100
  Vegetation IGBP: 	MF (Mixed Forests: Lands dominated by trees with a percent cover >60% and height    exceeding 2 meters. Consists of tree communities with interspersed mixtures or mosaics of the other   four forest types. None of the forest types exceeds 60% of landscape.) 


Differences: 
  Main difference is the elevation and connected to that the mean temperature and the types of          vegetation. In Davos there are mainly evergreen needle leaf forest whereas in Laegern there are mixed   forests.
  
Biases of the out-of-sample predictions:

If I compare the across-site predictions, I can observe that the Laegern model returns higher values that the Davos model. This might be the case because the Laegern model was trained with higher GPP values than the Davos model, due to the different biological conditions in the different sites (as shown above)