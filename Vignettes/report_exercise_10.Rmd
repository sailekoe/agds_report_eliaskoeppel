---
title: "report_exercise_10"
author: "Elias KÃ¶ppel"
date: "2023-05-20"
output: html_document
---

Proceed as follows:

Set aside 20% of the data of each site for testing.

Compare within-site predictions and across-site predictions on the test set for both sites, considering different metrics. For across-site predictions, make sure to implement a train and test setup that enables a true out-of-sample prediction test.

Train a single model with training data pooled from both sites and predict with this single model on the test data of both sites. How do the model metrics on the test set compare to the true out-of-sample setup above?
Interpret differences. Is it a valid approach to perform model training like this? Use your knowledge about structure in the data and its relevance for the model training setup.

Get information about the characteristics of the two sites. What are the differences in terms of climate, vegetation, altitude, etc. between the Davos and Laegern sites? Interpret biases of the out-of-sample predictions with a view to the site characteristics.



```{r}
library(dplyr)
library(tidyverse)
library(cowplot)
library(ggplot2)
library(recipes)
source("report_exercise_10_functions.R")




davos_flux <- read_csv("C:/Users/elias/OneDrive/Uni/4. Semester/Geo/AGDS I/agds_report_eliaskoeppel/data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv")

davos_flux <- get_selection(davos_flux)
davos_flux <- davos_flux[, -c(1,8)] #getting rid of P_F because I want to pool the davos data with the laegern data later and in the laegern data all the P_F values are NA. Also I get rid of the Timestamp column because in my laegern data set this coumn does not exist...

laegern_flux <- read_csv("C:/Users/elias/OneDrive/Uni/4. Semester/Geo/AGDS I/agds_report_eliaskoeppel/data/FLX_CH-Lae_FLUXNET2015_FULLSET_DD_2004-2014_1-4.csv")

laegern_flux <- get_selection_laegern(laegern_flux)
laegern_flux <- laegern_flux[,-7] #removing the seventh coulumn because it only consists of NA values

set.seed(1982) 


#getting splits for davos data
split1 <- rsample::initial_split(davos_flux, prop = 0.8, strata = "VPD_F")
  
davos_flux_train <- rsample::training(split1)
  
davos_flux_test <- rsample::testing(split1)


#getting splits for laegern data
split2 <- rsample::initial_split(laegern_flux, prop = 0.8, strata = "VPD_F")

laegern_flux_train <- rsample::training(split2)
  
laegern_flux_test <- rsample::testing(split2)


# pre-processing with the get_pp function I stored in the extra file
pp_davos_train <- get_pp(davos_flux_train)
pp_davos_test <- get_pp(davos_flux_test)

pp_laegern_train <- get_pp(laegern_flux_train)
pp_laegern_teset <- get_pp(laegern_flux_test)




```




k-fold cross validation for the davos_flux_train data:
```{r}
set.seed(1982)
mod_cv_davos_train <- caret::train(pp_davos_train, 
                       data = davos_flux_train |> drop_na(), 
                       method = "knn",
                       trControl = caret::trainControl(method = "cv", number = 10))
                       




```



k-fold cross validation for the laegern_flux_train data:
```{r}
set.seed(1982)
mod_cv_laegern_train <- caret::train(pp_laegern_train, 
                       data = laegern_flux_train |> drop_na(), 
                       method = "knn",
                       trControl = caret::trainControl(method = "cv", number = 10))
                       




```









How the cv model of the davos train set performs on the davos test set (within-site)

```{r}
plot_1 <- eval_model(mod = mod_cv_davos_train, df_test = davos_flux_test)

```


How the cv model of the davos train set performs on the laegern test set (across-site)
```{r}
plot_2 <- eval_model(mod = mod_cv_davos_train, df_test = laegern_flux_test)

```


In the following plot we see: 

On the left: how the model of the davos train set performs on the davos test set (within-site)
On the right: how the model of the davos train set performs on the laegern test set (across-site)
```{r}
cowplot::plot_grid(plot_1, plot_2) 

```
How the cv model of the laegern train set performs on the laegern test set (within-site)
```{r}
plot_3 <- eval_model(mod = mod_cv_laegern_train, df_test = laegern_flux_test)

```

How the cv model of the laegern train set performs on the davos test set (across-site)
```{r}
plot_4 <- eval_model(mod = mod_cv_laegern_train, df_test = davos_flux_test)

```







In the following plot we see: 

On the left: how the model of the laegern train set performs on the laegern test set (within-site)
On the right: how the model of the laegern train set performs on the davos test set (across-site)
```{r}
cowplot::plot_grid(plot_3, plot_4) 
```




pooling data and creating subsets, with which I create a single model
```{r}

data_pooled <- rbind(davos_flux, laegern_flux)




set.seed(1982) 


#getting splits for davos data
split3 <- rsample::initial_split(davos_flux, prop = 0.8, strata = "VPD_F")
  
data_pooled_train <- rsample::training(split3)
  
data_pooled_test <- rsample::testing(split3)





# pre-processing with the get_pp function I stored in the extra file
pp_data_pooled_train <- get_pp(davos_flux_train)
pp_data_pooled_test <- get_pp(davos_flux_test)




#creating model
set.seed(1982)
mod_data_pooled_train <- caret::train(pp_data_pooled_train, 
                       data = data_pooled_train |> drop_na(), 
                       method = "knn",
                       trControl = caret::trainControl(method = "cv", number = 10))




```





applying the pooled model to the test data sets of both sites
```{r}

plot_5 <- eval_model(mod = mod_data_pooled_train, df_test = davos_flux_test)

```


```{r}
plot_6 <- eval_model(mod = mod_data_pooled_train, df_test = laegern_flux_test)
```


In the following plot we see: 

On the left: how the model of the pooled data set performs on the davos test set
On the right: how the model of the pooled data set performs on the leagern test set 
```{r}
cowplot::plot_grid(plot_5, plot_6) 
```



The metrics I'm going to discuss:

The r-squared value measures how well the predictions made by a model match the actual data values. It is a statistical measure of how close the data points are to the fitted regression line. A higher r-squared value indicates a better fit of the data to the model.

The root mean square error (RMSE) value measures the average distance between the predicted values and the actual values. It is a measure of how well the model has performed in predicting the data, with lower RMSE values indicating better performance.
