---
title: "Report exercise 9"
author: "Elias KÃ¶ppel"
date: "2023-05-04"
output:
  html_document:
    toc: true
---

```{r, warning=FALSE}
source("report_exercise_9_functions.R") # accessing the functions I saved in a separate file called "report_exercise_9_functions"


library(ggplot2)
library(dplyr)
library(tidyverse)
library(rsample)
library(recipes)
library(caret)
library(class)
library(readr)
library(lubridate)
library(tidyr)
daily_fluxes <- read_csv("../data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv") 

daily_fluxes <- get_selection(daily_fluxes)



##use the functions I created in the file report_exercise_9_functions

  # creating the subsets
  daily_fluxes_train <- get_split(daily_fluxes)$train

  daily_fluxes_test <- get_split(daily_fluxes)$test

  #getting the pre-preocessed data
  pp <- get_pp(daily_fluxes)
  
  # getting the linear model and the knn model
  lmfit <- get_lm(data = daily_fluxes, pp = pp)

  knnfit <- get_knn(data = daily_fluxes, pp = pp)
```

```{r, warning=FALSE}
  # getting the evaluation of the regression model 
  eval_model(mod = lmfit, df_train = daily_fluxes_train, df_test = daily_fluxes_test)


```

```{r, warning=FALSE}
 # getting the evaluation of the knn model 
eval_model(mod = knnfit, df_train = daily_fluxes_train, df_test = daily_fluxes_test)
```

2) Interpret observed differences in the context of the bias-variance trade-off:

Why is the difference between the evaluation on the training and the test set larger for the KNN model than for the linear regression model?

    KNN is overfitting. --> high variance. That's the reason why the difference between the training set and the     testing set is bigger than for the linear regression model, which is probably underfitting, but therefore    has a very small variance. 

Why does the evaluation on the test set indicate a better model performance of the KNN model than the linear regression model?
  
    The KNN model (compared to the linear regression model) is a non-linear model, which can capture non-linear relationships between the input features and the target variable. So, the fact that the KNN model performs better at the test set than the linear regression model could mean that there is an non-linear relationship between the variables. 


How would you position the KNN and the linear regression model along the spectrum of the bias-variance trade-off?


    KNN --> here it depends a lot on the chosen k. If k is small, then we get a small bias and a big variance.             If k is big, however, we get a big bias and small variance. 
    
    linear model --> big bias, small variance, because it is so simple and therefore can not adapt to a lot of     nuances. 
    
    
3.) Visualise temporal variations of observed and modelled GPP for both models, covering all available dates   
    
    
First, I have to get the fitted values for the different models and subsets. Then I'm going to visualise all of them in one plot. 

modelled GPP values with the lm model on the train set
```{r, warning=FALSE}

#fist I have to get the modeled GPP values (therefore I copied a part of the eval_model function)

df_train_lm <- daily_fluxes_train |> #creating a new data set, which does not contain the NA values
    drop_na()

df_train_lm$fitted <- predict(lmfit, newdata = df_train_lm) #here a new column for the fitted value is created 
  


  
```
  
  
  
modelled GPP values with the KNN model on the train set
```{r, warning=FALSE}

#fist I have to get the modelled GPP values (I copied a part of the eval_model function)

df_train_knn <- daily_fluxes_train |> #creating a new data set, which does not contain the NA values
    drop_na()

df_train_knn$fitted <- predict(knnfit, newdata = df_train_knn) #here a new column for the fitted value is created 
  


  
```

modelled GPP values with the KNN model on test set
```{r, warning=FALSE}

#fist I have to get the modelled GPP values (I copied a part of the eval_model function)

df_test_knn <- daily_fluxes_test |> #creating a new data set, which does not contain the NA values
    drop_na()

df_test_knn$fitted <- predict(knnfit, newdata = df_test_knn) #here a new column for the fitted value is created 
  

  
```

modelled GPP values with the lm model on test set
```{r, warning=FALSE}

#fist I have to get the modeled GPP values (I copied a part of the eval_model function)

df_test_lm <- daily_fluxes_test |> #creating a new data set, which does not contain the NA values
    drop_na()

df_test_lm$fitted <- predict(lmfit, newdata = df_test_lm) #here a new column for the fitted value is created 
  

  
```


```{r, warning=FALSE}
ggplot(data = df_train_lm, aes(x = TIMESTAMP, y = fitted)) +
  geom_point(aes(color = "LM on training set")) +
  geom_point(data = df_test_lm, aes(x = TIMESTAMP, y = fitted, color = "LM on test set")) +
  geom_point(data = df_train_knn, aes(x = TIMESTAMP, y = fitted, color = "KNN on training set")) +
  geom_point(data = df_test_knn, aes(x = TIMESTAMP, y = fitted, color = "KNN on test set")) +
  geom_point(data = df_train_lm, aes(x = TIMESTAMP, y = GPP_NT_VUT_REF, color = "observed GPP")) +
  scale_color_manual("Data Type", values = c("LM on training set" = "orange", "LM on test set" = "red", 
                                "KNN on training set" = "blue", "KNN on test set" = "black", 
                                "observed GPP" = "green")) +
  theme_classic()
```


  
  
exercise: Let's look at the role of k in a KNN. Answer the following questions:

Based on your understanding of KNN (and without running code), state a hypothesis for how the R2
and the MAE evaluated on the test and on the training set would change for k approaching 1 and for k approaching N (the number of observations in the data). Explain your hypothesis, referring to the bias-variance trade-off.
      
    If k approaches 1: the model has low bias but higher variance. This is because the model is considering only the nearest neighbor which could result in overfitting. Therefore, I would expect the R2 on the training set to increase while the MAE would decrease. On the test set, however, we could see a decrease in the R2 as the model would not generalize well to new data resulting in higher prediction errors -->  the MAE increases as well.

    If k approaches N: the model has high bias but lower variance. Considering more neighbors should reduce the overfitting to the training data but might result in underfitting if the model is too simple. Therefore, I would expect the R2 on the training set to decrease while the MAE would increase. However, on the test set, I would expect the model to generalize better to new data with lower prediction errors --> the R2  increases and the MAE decreases.


Put your hypothesis to the test! Write code that splits the data into a training and a test set and repeats model fitting and evaluation for different values for k. Visualize results, showing model generalisability as a function of model complexity. Describe how a region of overfitting and underfitting can be determined in your visualization. Write (some of your) code into a function that takes k as an input and and returns the MAE determined on the test set.


Is there an optimal k in terms of model generalisability? Edit your code to determine an optimal k.
    
    
  
```{r, warning=FALSE}

# I don't have to split the data again because it is already splited from the first part of the exercise. 



## getting 40 models for k[1:40] with the function k_models I created
k_models <- get_k_models(daily_fluxes_train, 40) 



mae_test <- list()
for (i in 1:length(k_models)){
mae_test[[i]] <- eval_model2(mod = k_models[[i]], df_train = daily_fluxes_train, df_test = daily_fluxes_test)} 




mae_train <- list()
for (i in 1:length(k_models)){
mae_train[[i]] <- eval_model3(mod = k_models[[i]], df_train = daily_fluxes_train, df_test = daily_fluxes_test)} 



# Putting the mae values into a data frame
df <- tibble(
  mae_test = unlist(mae_test),
  mae_train = unlist(mae_train))

df <- data.frame(cbind(unlist(mae_test), 
                       unlist(mae_train)))


# add column that numbers the lines
df <- df %>% 
  mutate(row_number= row_number())




# plotting the results
ggplot(data=df, aes(x = row_number)) +
  geom_point(aes(y = X1, color = "X1"), show.legend = TRUE) + 
  geom_point(aes(y = X2, color = "X2"), show.legend = TRUE) + 
  scale_color_manual(values = c("X1" = "blue", "X2" = "red"), 
                     labels = c("MAE values for TEST set", "MAE values for TRAIN set")) +
  xlab("number of k") +
  ylab("mae values")



```


The optimal k in terms of model generalisability would be at the point where the MAE value for the test set (blue dots) is at its minimum: 

```{r, warning=FALSE}
which.min(df[,1])
```


In this case the minimal MAE is achieved with k = 19.  

